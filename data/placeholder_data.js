// Store language strings here
const translations = {
    en: {
        site_title: "Storytelling Benchmark",
        nav_overview: "Overview",
        nav_leaderboard_detail: "Detailed Leaderboard",
        nav_metrics: "Metrics Definition",
        nav_dataset: "Dataset",
        nav_explorer: "Result Explorer",
        overview_title: "Storytelling Benchmark Overview & Leaderboard",
        leaderboard_detail_title: "Detailed Leaderboard",
        intro_goal_title: "Goal, Value, and Core Findings",
        intro_goal_text: "[Placeholder EN: Briefly describe the objective of this benchmark - e.g., to comprehensively evaluate image generation models for storytelling tasks. Explain the value - e.g., providing insights into model capabilities in consistency, adherence, and quality. Mention 1-2 key findings - e.g., Model X excels in character consistency, while Model Y shows better prompt adherence.]",
        leaderboard_title: "Leaderboard Summary",
        leaderboard_detail_results: "Results",
        leaderboard_expand_hint: "Click the '+' icon to view detailed sub-metric scores.",
        table_model: "Model/Method",
        table_timestamp: "Timestamp",
        table_dataset: "Dataset",
        table_mode: "Mode",
        table_cref: "CRef",
        table_sref: "SRef",
        table_prompt: "Prompt Adh.",
        table_quality: "Quality",
        table_overall: "Overall",
        table_notes: "Notes / Link",
        table_details: "Details",
        view_details: "View Details",
        news_title: "News & Updates",
        news_item_1: "[Placeholder EN: Benchmark v1.0 launched!]",
        news_item_2: "[Placeholder EN: Added new model 'XYZ' to the evaluation.]",
        resources_title: "Resources",
        resources_code: "[Placeholder EN: Link to Benchmark Code (GitHub)]",
        resources_paper: "[Placeholder EN: Link to Related Paper (arXiv/etc.)]",
        resources_dataset: "[Placeholder EN: Link to Dataset Details Page]",
        about_title: "About",
        about_citation_title: "Citation",
        about_citation_text: "[Placeholder EN: If you use this benchmark in your research, please cite it as follows:]",
        about_license_title: "License",
        about_license_text: "[Placeholder EN: The benchmark code and results are licensed under the [License Name, e.g., Apache 2.0] License. The dataset might have its own license.]",
        footer_text: "Storytelling Benchmark © 2025",
        // --- Model Detail Page --- (Add keys as needed)
        model_detail_title: "Model Details: [Model Name]",
        overall_stats: "Overall Statistics",
        stories_evaluated: "Stories Evaluated",
        avg_score: "Average Score",
        // --- Story Detail Page (Result Explorer) --- (Add keys as needed)
        story_detail_title: "Result Explorer: Story [Story ID]",
        input_script: "Input Script",
        character_ref: "Character Reference",
        generated_output: "Generated Output for [Model Name]",
        select_models: "Compare Models:",
        shot: "Shot",
        shot_current_of_total: "Shot [Current Shot] of [Total Shots]",
        prev_shot: "Previous Shot",
        next_shot: "Next Shot",
        // --- Metrics Page --- (Add keys as needed)
        metrics_title: "Benchmark Metrics Definition",
        cref_title: "Character Consistency",
        cref_desc: "Evaluates if characters look consistent across image sequences.",
        cref_sub_accs: "aCCS (Avg. Char Similarity)",
        cref_sub_accs_desc: "[EN Placeholder: Uses CLIP image features to calculate similarity across frames (Ref: Theatregen, story-adapter). Suitable for general characters.]",
        cref_sub_face: "ArcFace / AdaFace",
        cref_sub_face_desc: "[EN Placeholder: Specialized facial recognition models for realistic human characters.]",
        cref_sub_ccip: "CCIP",
        cref_sub_ccip_desc: "[EN Placeholder: Suitable for anime-style characters, requires single character per image detection.]",
        cref_sub_confusion: "Multi-Person Confusion",
        cref_sub_confusion_desc: "[EN Placeholder: Measures distinguishability in multi-character scenes.]",
        cref_sub_grounding: "Grounding DINO + Similarity",
        cref_sub_grounding_desc: "[EN Placeholder: Detects main subjects then calculates CLIP/FID/DreamSim similarity.]",
        cref_sub_dreamsim: "DreamSim Similarity",
        cref_sub_dreamsim_desc: "[EN Placeholder: Cross-modal fine-grained matching metric.]",
        cref_sub_vbench: "VBench Character Tracking",
        cref_sub_vbench_desc: "[EN Placeholder: Adapts video consistency metrics for image sequences.]",
        cref_sub_charf1: "Char-F1 / Char-Acc",
        cref_sub_charf1_desc: "[EN Placeholder: Uses pre-trained classifiers to identify characters and compare with Ground Truth.]",
        cref_sub_vlm: "VLM Validation",
        cref_sub_vlm_desc: "[EN Placeholder: Compares generated characters against human-annotated Ground Truth using VLMs.]",
        sref_title: "Style Consistency",
        sref_desc: "Evaluates if the artistic style is consistent.",
        prompt_title: "Prompt Adherence",
        prompt_desc: "Evaluates how well the generated images follow the text prompt.",
        quality_title: "Generation Quality",
        quality_desc: "Assesses the visual quality and aesthetics of the images.",
        diversity_title: "Diversity",
        diversity_desc: "Measures the variety in generated outputs.",
        human_eval_title: "Human Evaluation",
        human_eval_desc: "Subjective ratings from human evaluators.",
        // --- Dataset Page --- (Add keys as needed)
        dataset_title: "Benchmark Dataset Information",
        dataset_desc: "Details about the dataset used for evaluation.",
        dataset_source: "Source: [Dataset Source]",
        dataset_stats: "Statistics: [Number] stories, Avg. [Number] shots per story."
    },
    zh: {
        site_title: "Storytelling Benchmark",
        nav_overview: "概览",
        nav_leaderboard_detail: "详细排行榜",
        nav_metrics: "指标定义",
        nav_dataset: "数据集",
        nav_explorer: "结果浏览器",
        overview_title: "Storytelling Benchmark 概览与排行榜",
        leaderboard_detail_title: "详细排行榜",
        intro_goal_title: "目标、价值与核心发现",
        intro_goal_text: "[中文占位符: 简述 Benchmark 目标 - 例如，全面评估图像生成模型在故事叙述任务上的表现。解释其价值 - 例如，深入了解模型在一致性、遵循度、质量等方面的能力。提及1-2个核心发现 - 例如，模型X在角色一致性上表现优异，而模型Y在Prompt遵循度上更佳。]",
        leaderboard_title: "排行榜摘要",
        leaderboard_detail_results: "结果",
        leaderboard_expand_hint: "点击 '+' 图标查看详细子项得分。",
        table_model: "模型/方法",
        table_timestamp: "时间戳",
        table_dataset: "数据集",
        table_mode: "模式",
        table_cref: "角色一致性",
        table_sref: "风格一致性",
        table_prompt: "Prompt遵循度",
        table_quality: "生成质量",
        table_overall: "综合得分",
        table_notes: "备注/链接",
        table_details: "详情",
        view_details: "查看详情",
        news_title: "新闻与更新",
        news_item_1: "[中文占位符: Benchmark v1.0 发布！]",
        news_item_2: "[中文占位符: 新增模型 'XYZ' 加入评估。]",
        resources_title: "资源",
        resources_code: "[中文占位符: Benchmark 代码链接 (GitHub)]",
        resources_paper: "[中文占位符: 相关论文链接 (arXiv/等)]",
        resources_dataset: "[中文占位符: 数据集详情页面链接]",
        about_title: "关于",
        about_citation_title: "引用",
        about_citation_text: "[中文占位符: 如果您在研究中使用了本 Benchmark，请按以下方式引用:]",
        about_license_title: "许可证",
        about_license_text: "[中文占位符: Benchmark 代码和结果采用 [许可证名称，例如 Apache 2.0] 许可证。数据集可能有其自身的许可证。]",
        footer_text: "Storytelling Benchmark © 2025",
        // --- Model Detail Page --- (Add keys as needed)
        model_detail_title: "模型详情: [Model Name]",
        overall_stats: "总体统计",
        stories_evaluated: "已评估故事",
        avg_score: "平均得分",
        // --- Story Detail Page (Result Explorer) --- (Add keys as needed)
        story_detail_title: "结果浏览器: 故事 [Story ID]",
        input_script: "输入剧本",
        character_ref: "角色参考",
        generated_output: "模型 [Model Name] 的生成结果",
        select_models: "对比模型:",
        shot: "镜头",
        shot_current_of_total: "镜头 [Current Shot] / [Total Shots]",
        prev_shot: "上一镜头",
        next_shot: "下一镜头",
        // --- Metrics Page --- (Add keys as needed)
        metrics_title: "Benchmark 指标定义",
        cref_title: "角色一致性",
        cref_desc: "评估角色在图像序列中的外观是否一致。",
        cref_sub_accs: "aCCS (平均角色相似度)",
        cref_sub_accs_desc: "[中文占位符: 使用 CLIP 图像特征计算跨帧相似度 (来源: Theatregen, story-adapter)。适用于通用角色。]",
        cref_sub_face: "ArcFace / AdaFace",
        cref_sub_face_desc: "[中文占位符: 适用于真人角色的专用人脸识别模型。]",
        cref_sub_ccip: "CCIP",
        cref_sub_ccip_desc: "[中文占位符: 适用于二次元角色，需单图单角色检测。]",
        cref_sub_confusion: "多人混淆度",
        cref_sub_confusion_desc: "[中文占位符: 评估多角色场景下的区分度。]",
        cref_sub_grounding: "Grounding DINO + 相似度",
        cref_sub_grounding_desc: "[中文占位符: 检测主体后，使用 CLIP/FID/DreamSim 计算相似度。]",
        cref_sub_dreamsim: "DreamSim 相似度",
        cref_sub_dreamsim_desc: "[中文占位符: 跨模态细粒度匹配指标。]",
        cref_sub_vbench: "VBench 角色跟踪",
        cref_sub_vbench_desc: "[中文占位符: 将视频角色一致性指标迁移用于图像序列。]",
        cref_sub_charf1: "Char-F1 / Char-Acc",
        cref_sub_charf1_desc: "[中文占位符: 使用预训练分类器识别角色并与 GT 对比计算 F1 和准确率。]",
        cref_sub_vlm: "VLM 验证",
        cref_sub_vlm_desc: "[中文占位符: 对比人工标注的角色与生成结果的一致性 (需要 GT)。]",
        sref_title: "风格一致性",
        sref_desc: "评估艺术风格是否保持一致。",
        prompt_title: "Prompt 遵循度",
        prompt_desc: "评估生成图像遵循文本提示的程度。",
        quality_title: "生成质量",
        quality_desc: "评估图像的视觉质量和美观度。",
        diversity_title: "多样性",
        diversity_desc: "衡量生成输出的多样性。",
        human_eval_title: "人工评估",
        human_eval_desc: "来自人类评估者的主观评分。",
        // --- Dataset Page --- (Add keys as needed)
        dataset_title: "Benchmark 数据集信息",
        dataset_desc: "关于用于评估的数据集的详细信息。",
        dataset_source: "来源: [数据集来源]",
        dataset_stats: "统计: [数量] 个故事, 平均每个故事 [数量] 个镜头。"
    }
}; 